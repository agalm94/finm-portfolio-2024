{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "from scipy.stats import norm, chi2\n",
    "import statsmodels.api as sm\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.abspath(os.path.join(current_dir, os.pardir))\n",
    "grandparent_dir = os.path.abspath(os.path.join(parent_dir, os.pardir))\n",
    "sys.path.insert(0, parent_dir)\n",
    "sys.path.insert(0, grandparent_dir)\n",
    "import cmds.portfolio_management_helper as pmh\n",
    "\n",
    "plt.style.use(\"seaborn-v0_8-whitegrid\")\n",
    "PLOT_WIDTH, PLOT_HEIGHT = 8, 5\n",
    "COLORS = [\"blue\", \"red\", \"orange\"]\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.options.display.float_format = \"{:.4f}\".format\n",
    "p = plt.rcParams\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = parent_dir + '/data/'\n",
    "FILE_NAME = DATA_PATH + 'momentum_data.xlsx'\n",
    "excess_ff_factors = pmh.read_excel_default(FILE_NAME, \n",
    "                                 sheet_name='factors (excess returns)',\n",
    "                                 index_col='Date', parse_dates=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Timeframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pmh.calc_summary_statistics(factors, annual_factor=12, correlations=False, provided_excess_returns=True,\n",
    "                            keep_columns=['Annualized'], drop_columns=['VaR']).T.style.format(\"{:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Timeframes (with Correlations!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_table = pmh.calc_summary_statistics(ff_factors, annual_factor=12, provided_excess_returns=True, \n",
    "                            timeframes={'1927-2024':['1927', '2024'],\n",
    "                                        '1927-1993':['1927', '1993'],\n",
    "                                        '1994-2008':['1994', '2008'],\n",
    "                                        '2009-2024':['2009', '2024']},\n",
    "                            correlations=['MKT', 'HML'],\n",
    "                            keep_columns=['Annualized Mean', 'Annualized Vol', 'Annualized Sharpe', 'Skewness', 'Correlation'])\n",
    "summary_table.loc[summary_table.index.str.contains('UMD')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mean-Variance Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tangency Portfolio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classic Tangency Weights \n",
    "pmh.calc_tangency_weights(rets, annual_factor=ANNUAL_FACTOR)\n",
    "\n",
    "# Tangency Weights with a target Return\n",
    "pmh.calc_tangency_weights(rets, target_ret_rescale_weights=0.0025, annual_factor=ANNUAL_FACTOR)\n",
    "\n",
    "# Tangency Weights with a regularized Covariance Matrix\n",
    "pmh.calc_tangency_weights(rets, cov_mat=COV_MAT)\n",
    "\n",
    "# Tangency Portfolio with different return series for expected returns and covariance matrix\n",
    "training_tangency_weights_expected_returns = pmh.calc_tangency_weights(\n",
    "    training_stocks_excess_returns, # Series used to calculate the covriance matrix\n",
    "    cov_mat=COV_MATRIX,\n",
    "    expected_returns=stocks_expected_excess_return, # Series used to calculate the expected returns\n",
    "    name=\"AQR Expected Returns\"\n",
    ")\n",
    "training_tangency_weights_expected_returns.iloc[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GMV Portfolio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classic GMV weights\n",
    "pmh.calc_gmv_weights(rets, name='MV')\n",
    "\n",
    "# GMV weights with a target Return\n",
    "pmh.calc_gmv_weights(rets, target_ret_rescale_weights=0.0025, name='MV')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Equal Weights Portfolio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pmh.calc_equal_weights(rets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining Portfolios and Generating Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IS_RETS = rets.loc[:'2021'].drop(columns=['BTC'])\n",
    "OOS_RETS = rets.loc['2022':'2023'].drop(columns=['BTC'])\n",
    "\n",
    "no_btc_wts = pmh.calc_tangency_weights(IS_RETS, annual_factor=ANNUAL_FACTOR,target_ret_rescale_weights=0.0025)\n",
    "equal_wts = pmh.calc_equal_weights(IS_RETS)\n",
    "\n",
    "target_portfolio = pmh.create_portfolio(OOS_RETS, weights=no_btc_wts.iloc[:, 0].to_dict(), port_name='Target')\n",
    "equal_portfolio = pmh.create_portfolio(OOS_RETS, weights=equal_wts.iloc[:, 0].to_dict(), port_name='Equal')\n",
    "\n",
    "pmh.calc_summary_statistics(pd.concat([target_portfolio, equal_portfolio], axis=1), annual_factor=ANNUAL_FACTOR, \n",
    "                            provided_excess_returns=True, keep_columns=['Annualized'], drop_columns='VaR').T.style.format(\"{:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Factor Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Time Series Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For any LFPM task, need to account for IS and OOS periods when setting up the regression and testing performance\n",
    "LAST_IN_SAMPLE_YEAR = \"2018\"\n",
    "FIRST_OUT_OF_SAMPLE_YEAR = f\"{(int(LAST_IN_SAMPLE_YEAR) + 1):.0f}\"\n",
    "COV_MATRIX = .5\n",
    "\n",
    "excess_returns_in_sample = excess_returns.loc[:LAST_IN_SAMPLE_YEAR]\n",
    "excess_returns_out_of_sample = excess_returns.loc[FIRST_OUT_OF_SAMPLE_YEAR:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Dependent Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single dependent variable regressions are ususally used for LFD and Hedging tasks\n",
    "# Return a summary table\n",
    "pmh.calc_regression(excess_returns_in_sample, factors, include_intercept=True, annual_factor=ANNUAL_FACTOR)\n",
    "\n",
    "# Return the model for use in prediction\n",
    "model = pmh.calc_regression(excess_returns_in_sample, factors, include_intercept=True, annual_factor=ANNUAL_FACTOR, return_model=True)\n",
    "model.predict(sm.add_constant(excess_returns_out_of_sample))    # Critical that you add the constant IF INTERCEPT IS INCLUDED\n",
    "model.params[1:]    # Beta coefficients for hedging and replication problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterative Regression (Multiple Dependent Variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is typically used for the time-series regression step in LFPM tasks\n",
    "capm_ts = pmh.calc_iterative_regression(rets.loc['1981':], factors.loc['1981':],\n",
    "                              warnings=False,\n",
    "                              keep_columns=['Alpha', 'Beta', 'R-Squared',\n",
    "                                            'Annualized'])\n",
    "display(capm_ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Absolute Error Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** This MAE test is different from the CS MAE test because we measure the MAE of the *alphas* across the TS regressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame((capm_ts['Annualized Alpha']).abs().mean(), columns = ['MAE (%)'], index = ['CAPM'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fama-French 5-Factor Test\n",
    "FF5F = ['MKT', 'SMB', 'HML', 'RMW', 'CMA']\n",
    "ff5f_ts_test = pmh.calc_iterative_regression(portfolios, factors[FF5F], annual_factor=12,intercept=True, \n",
    "                                            keep_columns=['Annualized Alpha', 'R-Squared'])\n",
    "display(ff5f_ts_test)\n",
    "print(f'Mean-Absolute-Error: {ff5f_ts_test['Annualized Alpha'].abs().sum() / len(ff5f_ts_test):.2%}\\\n",
    "      \\nMin-Absolute-Error: {ff5f_ts_test['Annualized Alpha'].abs().idxmin()} - {ff5f_ts_test['Annualized Alpha'].abs().min():.2%}\\\n",
    "      \\nMax-Absolute-Error: {ff5f_ts_test['Annualized Alpha'].abs().idxmax()} - {ff5f_ts_test['Annualized Alpha'].abs().max():.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Series Regression with Time-Varying Betas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is used to assess the accuracy of a replication strategy while the model is re-trained through time\n",
    "def OOS_strat(df, factors, start, window_size=None, intercept=True):\n",
    "    y = df\n",
    "    if intercept:\n",
    "        X = sm.add_constant(factors)\n",
    "    else:\n",
    "        X = factors\n",
    "\n",
    "    forecast_err, null_err,oos_predictions,null_predictions = [], [],[],[]\n",
    "\n",
    "    for i,j in enumerate(df.index):\n",
    "        if i >= start:\n",
    "            if window_size:\n",
    "                begin = i - window_size\n",
    "            else:\n",
    "                begin = 0\n",
    "            currX = X.iloc[begin:i]\n",
    "            currY = y.iloc[begin:i]\n",
    "            reg = sm.OLS(currY, currX, missing = 'drop').fit()\n",
    "            null_forecast = currY.mean()\n",
    "            reg_predict = reg.predict(X.iloc[[i]])\n",
    "            actual = y.iloc[[i]]\n",
    "            oos_predictions.append(reg_predict.T)\n",
    "            null_predictions.append(pd.DataFrame([[reg_predict.index[0]]], columns = ['date'], index = [null_forecast]))\n",
    "            forecast_err.append(reg_predict.values - actual)\n",
    "            null_err.append(null_forecast.values - actual)\n",
    "            \n",
    "    RSS = (np.array(forecast_err)**2).sum()\n",
    "    TSS = (np.array(null_err)**2).sum()\n",
    "    predictions_df = pd.concat(oos_predictions).T.drop_duplicates()\n",
    "    null_predictions_df = pd.concat(null_predictions).T\n",
    "    \n",
    "    return ((1 - RSS/TSS),reg,predictions_df,null_predictions_df)\n",
    "\n",
    "# Example of how to use the function\n",
    "OOS_r2, OOS_model, OOS_forecasts, null_predictions_df = OOS_strat(USO,features, features.loc[:'2017'].shape[0])\n",
    "OOS_pred = OOS_forecasts.to_frame('OOS Forecast')\n",
    "OOS_pred\n",
    "\n",
    "print(f'Out-of-sample R-squared: {OOS_r2:.2%}')\n",
    "\n",
    "corr = pmh.calc_correlations(pd.concat([OOS_pred, USO.loc['2018':'2023']], axis=1), \n",
    "                             return_heatmap=False, print_highest_lowest=False).iloc[0, 1]\n",
    "print(f'Correlation between the out-of-sample forecast and USO: {corr:.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Sectional Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Absolute Error Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** This MAE test is different from the TS MAE test because we measure MAE in the cross-sectional regression as the sum of *error residuals*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capm_cs_test = pmh.calc_cross_section_regression(portfolios, factors['MKT'].to_frame(),provided_excess_returns=True, annual_factor=12, \n",
    "                                                 name='CAPM',keep_columns=['R-Squared', 'Annualized Eta', 'Annualized Lambda', \n",
    "                                                                           'TS Annualized MAE', 'CS Annualized MAE']).T\n",
    "aqr_cs_test = pmh.calc_cross_section_regression(portfolios, factors[AQR],provided_excess_returns=True, annual_factor=12, \n",
    "                                                 name='AQR',keep_columns=['R-Squared', 'Annualized Eta', 'Annualized Lambda', \n",
    "                                                                           'TS Annualized MAE', 'CS Annualized MAE']).T\n",
    "ff3f_cs_test = pmh.calc_cross_section_regression(portfolios, factors[FF3F],provided_excess_returns=True, annual_factor=12, \n",
    "                                                 name='FF3F',keep_columns=['R-Squared', 'Annualized Eta', 'Annualized Lambda', \n",
    "                                                                           'TS Annualized MAE', 'CS Annualized MAE']).T\n",
    "ff5f_cs_test = pmh.calc_cross_section_regression(portfolios, factors[FF5F],provided_excess_returns=True, annual_factor=12, \n",
    "                                                 name='FF5F',keep_columns=['R-Squared', 'Annualized Eta', 'Annualized Lambda', \n",
    "                                                                           'TS Annualized MAE', 'CS Annualized MAE']).T\n",
    "pd.concat([capm_cs_test, aqr_cs_test, ff3f_cs_test, ff5f_cs_test], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bivariate Significance Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating Residuals Covariance Matrix\n",
    "resid = pd.DataFrame()\n",
    "for pf in pf_excess_rets.columns:\n",
    "    r = pmh.calc_regression(pf_excess_rets.loc['1981':, pf], factors.loc['1981':, 'Mkt-RF'].to_frame('Mkt-RF'), \n",
    "                            annual_factor=12, warnings=False, return_model=True, return_fitted_values=False)\n",
    "    r = r.resid.to_frame(pf)\n",
    "    resid = pd.concat([resid, r], axis=1)\n",
    "\n",
    "# Conducting the H- and t-tests (requires residuals from above and alphas from iterative regression)\n",
    "T = pf_excess_rets['1981':].shape[0]\n",
    "SR = (factors['1981':]['Mkt-RF'].mean() / factors['1981':]['Mkt-RF'].std()) #* np.sqrt(12)\n",
    "Sigma = resid.cov()\n",
    "Sigma_inv = pd.DataFrame(np.linalg.inv(Sigma), index=Sigma.index, columns=Sigma.columns)\n",
    "alpha = capm_ts['Alpha']    # Not Annualized\n",
    "\n",
    "H = T * (1 + SR**2)**(-1) * (alpha @ Sigma_inv @ alpha)\n",
    "\n",
    "print('H = {:.2f}'.format(H))\n",
    "pvalue = 1 - chi2.cdf(H, df=25)\n",
    "print('p-value = {:.4f}'.format(pvalue))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Diversification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability of Under Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob_under(mu, sigma, c, h):\n",
    "    return norm.cdf(((c-mu)/sigma) * np.sqrt(h))\n",
    "\n",
    "mu = barn_rets.loc['1965':'1999', 'LOG_SPX-XS'].mean()\n",
    "sigma = barn_rets.loc['1965':'1999', 'LOG_SPX-XS'].std()\n",
    "\n",
    "# NOTE: We are using 0 for the c parameter here because we are using excess returns\n",
    "print(f'From 1965-1999, Monthly:\\n\\tPr(SPX Returns < RF Returns) = {prob_under(mu, sigma, c=0, h=1):.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(0, 12*30)\n",
    "y = prob_under(mu, sigma, c=0, h=x)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x, y)\n",
    "ax.set_title('Change in Probability of Under-Performance\\nover Different Time Horizons\\n(1965-1999)')\n",
    "ax.set(xlabel='Holding Period\\n(Months)', ylabel='Pr(SPX < RF)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This example is assessing probability of outperformance. Trick: Subtract the probability from 1\n",
    "mu = aqr_log_rets.loc['2009':, 'UMD'].mean()\n",
    "sigma = aqr_log_rets.loc['2009':, 'UMD'].std()\n",
    "c = aqr_log_rets.loc[:, 'MKT'].mean()\n",
    "\n",
    "print(f'Single Period:\\n\\tPr(UMD Mean Excess Rets > MKT) = {1-prob_under(mu, sigma, c=c, h=1):.2%}')\n",
    "print(f'15-Year:\\n\\tPr(UMD Mean Excess Rets > MKT) = {1-prob_under(mu, sigma, c=c, h=12*15):.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing Time Diversification on Returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pmh.calc_summary_statistics(aqr_log_rets, annual_factor=12*15, # This annual factor scales the log returns for time diversification analysis\n",
    "                            provided_excess_returns=True,\n",
    "                            keep_columns=['Annualized Mean', 'Annualized Vol', 'Annualized Sharpe']).T.style.format('{:.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: The features parameter in the below is shifted down to make this forecasting regression\n",
    "model = pmh.calc_regression(target, features, annual_factor=ANNUAL_FACTOR, return_model=True)\n",
    "pmh.calc_regression(target, features, annual_factor=ANNUAL_FACTOR,\n",
    "                    keep_columns=['R-Squared', 'Alpha', 'Beta'], \n",
    "                    drop_columns=['Annualized']).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wts = 0.5 + 50 * model.predict(sm.add_constant(features))\n",
    "wts = pd.DataFrame(wts, wts.index, ['Weights'])\n",
    "rets = pd.DataFrame(target.iloc[1:].values * wts.values, wts.index, ['Strat Returns'])\n",
    "rets.style.format(\"{:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rolling OOS Forecasting Performance\n",
    "OOS_r2, OOS_reg_params, OOS_forecasts, null_predictions_df = OOS_strat(USO,features, features.loc[:'2017'].shape[0])\n",
    "OOS_pred = OOS_forecasts.to_frame('OOS Forecast')\n",
    "OOS_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = pmh.calc_correlations(pd.concat([OOS_pred, USO.loc['2018':'2023']], axis=1), \n",
    "                             return_heatmap=False, print_highest_lowest=False).iloc[0, 1]\n",
    "print(f'Correlation between the out-of-sample forecast and USO: {corr:.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carry Trade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Static"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spot_returns = log_ex_rates - log_ex_rates.shift(1)\n",
    "rate_returns = log_rf_rates.drop('USD', axis=1) - log_rf_rates[['USD']].values\n",
    "xs_log_rets = spot_returns + rate_returns\n",
    "\n",
    "(pmh.calc_summary_statistics(xs_log_rets, annual_factor=252, provided_excess_returns=True,\n",
    "                            keep_columns=['Annualized Mean', 'Annualized Vol', 'Annualized Sharpe',])\n",
    "                            .T.style.format(\"{:.2%}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyzing the Carry Trade Performance\n",
    "mean = xs_log_ret.mean().values[0] * ANNUAL_FACTOR\n",
    "spread = rate_parity.mean() * ANNUAL_FACTOR\n",
    "curr_diff = spot_diff.sum().values[0]\n",
    "pd.DataFrame(\n",
    "            [mean, -spread, curr_diff], ['Mean Excess Log Returns', 'Mean Rate Spread', 'Spot Difference'], ['Values']\n",
    "             ).style.format(\"{:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic - Using Regression to Estimate Expected Returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factor = -rate_returns.shift(1)\n",
    "regressions = {}\n",
    "for currency in spot_returns.columns:\n",
    "    regressions[currency] = pmh.calc_regression(spot_returns[[currency]], factor[[currency]], annual_factor=252, \n",
    "                                                intercept=True, return_model=True, warnings=False)\n",
    "    \n",
    "pd.DataFrame(columns=xs_log_rets.columns, index=['Annualized Alpha', 'Beta', 'R-Squared'], \n",
    "             data=[[regressions[currency].params[0] * 252 for currency in spot_returns.columns],\n",
    "                   [regressions[currency].params[1] for currency in spot_returns.columns],\n",
    "                   [regressions[currency].rsquared for currency in spot_returns.columns]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1)\n",
    "pred_xs_log_rets = pd.DataFrame()\n",
    "for currency in spot_returns.columns:\n",
    "    pred_xs_log_rets[currency] = regressions[currency].params[0] + (regressions[currency].params[1] - 1) * factor[[currency]].dropna()\n",
    "\n",
    "pd.DataFrame(index=['Count - Daily Premium > 0', 'Count - Days', 'Frequency (%) - Positive Premium'],\n",
    "             columns=pred_xs_log_rets.columns,\n",
    "             data = [(pred_xs_log_rets > 0).sum().map(\"{:.0f}\".format), \n",
    "                     pred_xs_log_rets.count().map(\"{:.0f}\".format), \n",
    "                     (pred_xs_log_rets > 0).mean().map(\"{:.2%}\".format)]).T"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
